{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4163bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import *\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.schema import Document\n",
    "from haystack.nodes import DensePassageRetriever, FARMReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.utils import print_answers\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as urllib2\n",
    "from googlesearch import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44a9bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(urls):\n",
    "    \n",
    "    documents = []\n",
    "    full_text = \"\"\n",
    "\n",
    "    for url in urls:\n",
    "\n",
    "        hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "        req = urllib2.Request(url, headers = hdr)\n",
    "        page = urllib2.urlopen(req)\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "        ps = []\n",
    "\n",
    "        for i, paragraph in enumerate(soup.findAll(\"p\")):\n",
    "            p = paragraph.text.split()\n",
    "            if len(p) == 0 or p[-1][-1] not in \".!,:;\":\n",
    "                continue\n",
    "            ps.append(\" \".join(p))\n",
    "            \n",
    "        text = \"\\n\".join(ps)\n",
    "        \n",
    "        full_text += text\n",
    "        documents.append(Document(content = text))\n",
    "        \n",
    "    return documents, full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "242201ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are metrics of machine learning?\"\n",
    "urls = [url for url in search(query, tld=\"co.in\", num = 5, stop = 5, pause=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d1944ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")\n",
    "docs, text = create_docs(urls)\n",
    "document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96469464",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = DensePassageRetriever(document_store=document_store,\n",
    "                                query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "                                passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9c42566",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Updating Embedding:   0%|                                                                    | 0/23 [00:00<?, ? docs/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Create embeddings:   0%|          | 0/32 [00:00<?, ? Docs/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Documents Processed: 10000 docs [00:15, 664.11 docs/s]                                                                 \n"
     ]
    }
   ],
   "source": [
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c6255a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/roberta-base-squad2 were not used when initializing RobertaModel: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at deepset/roberta-base-squad2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "ML Logging is turned off. No parameters, metrics or artifacts will be logged to MLFlow.\n"
     ]
    }
   ],
   "source": [
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e0dc659",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34a346a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.23s/ Batches]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.79s/ Batches]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:11<00:00, 11.37s/ Batches]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:04<00:00,  4.70s/ Batches]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.08s/ Batches]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:20<00:00, 20.67s/ Batches]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.04s/ Batches]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.30s/ Batches]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.41s/ Batches]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.98s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "prediction = pipe.run(query = query, params={\"Retriever\": {\"top_k\": 10}, \"Reader\": {\"top_k\": 5}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6417996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rning is a domain of computer science with its base in computational mathematics and statistics. The machine is shown a ton of data and it learns the ',\n",
       " 'l may give you satisfying results when evaluated using a metric say accuracy_score but may give poor results when evaluated against other metrics such',\n",
       " 'ing problems, with the most talked about algorithms being k-means and hierarchical clustering, though other algorithms like Hidden Markov models, Self',\n",
       " 'revious successful applicants.[111][112] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical par',\n",
       " 'scenario to play the game Mario.\\nNow that we’ve discussed the three different categories of machine learning, it’s important to note that a lot of tim']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = [answer.context for answer in prediction[\"answers\"]]\n",
    "contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ece158b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computational mathematics and statistics',\n",
       " 'accuracy_score',\n",
       " 'k-means and hierarchical clustering',\n",
       " 'Responsible collection of data and documentation of algorithmic rules',\n",
       " 'three different categories']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = [answer.answer for answer in prediction[\"answers\"]]\n",
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4e59407d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {   'answer': 'tracking monetary frauds online',\n",
      "        'context': 'proving its potential to make cyberspace a secure place '\n",
      "                   'and tracking monetary frauds online is one of its '\n",
      "                   'examples. For example: Paypal is using ML fo'},\n",
      "    {   'answer': 'data mining, statistics, and modeling',\n",
      "        'context': 'to make predictions about the future. Techniques such as '\n",
      "                   'data mining, statistics, and modeling employ machine '\n",
      "                   'learning and artificial intelligence to '},\n",
      "    {   'answer': 'medical diagnosis, image processing, prediction, '\n",
      "                  'classification, learning association, regression',\n",
      "        'context': 'd industries. For example, medical diagnosis, image '\n",
      "                   'processing, prediction, classification, learning '\n",
      "                   'association, regression etc.\\n'\n",
      "                   '\\n'\n",
      "                   'The intelligent sys'},\n",
      "    {   'answer': 'tracking when you last walked your dog or notifying when '\n",
      "                  'your kids are back home from school',\n",
      "        'context': ' intriguing features such as tracking when you last walked '\n",
      "                   'your dog or notifying when your kids are back home from '\n",
      "                   'school. Some of the latest systems '},\n",
      "    {   'answer': 'moderating spam and content search to monetizing '\n",
      "                  'advertising and reducing the number of mailing lists',\n",
      "        'context': 'usiness operations, from moderating spam and content '\n",
      "                   'search to monetizing advertising and reducing the number '\n",
      "                   'of mailing lists. Not bad.\\n'\n",
      "                   '\\n'\n",
      "                   'Neural netwo'}]\n"
     ]
    }
   ],
   "source": [
    "print_answers(prediction, details=\"minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e70373e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace(\"etc.\", \"etc\")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c16b3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20791\n",
      "59100\n",
      "23363\n",
      "52119\n",
      "77590\n"
     ]
    }
   ],
   "source": [
    "synth_text = \"\"\n",
    "text = \"\".join([p.content for p in prediction[\"documents\"]])\n",
    "\n",
    "for context in contexts:\n",
    "    target = text.find(context)\n",
    "    print(target)\n",
    "    start, end = target, target\n",
    "\n",
    "    while start >= 0:\n",
    "        if text[start] == \".\" or text[start] == \"\\n\":\n",
    "            start += 1\n",
    "            break\n",
    "        start -= 1\n",
    "\n",
    "    while end < len(text):\n",
    "        if (text[end] == \".\" or text[start] == \"\\n\") and (end - target) > len(context):\n",
    "            end += 1\n",
    "            break\n",
    "        end += 1\n",
    "\n",
    "    synth_text += text[start:end] + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f727a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>What are metrics of machine learning?</h3>\n",
       "<ul><li>computational mathematics and statistics</li><li>accuracy_score</li><li>k-means and hierarchical clustering</li><li>Responsible collection of data and documentation of algorithmic rules</li><li>three different categories</li></ul>\n",
       "<p>Machine Learning is a domain of computer science with its base in computational mathematics and statistics. The machine is shown a ton of data and it learns the pattern in the data to make future predictions, recognise new patterns or suggest different classes to the data.  Your model may give you satisfying results when evaluated using a metric say accuracy_score but may give poor results when evaluated against other metrics such as logarithmic_loss or any other such metric.  The most common use of unsupervised learning is in clustering problems, with the most talked about algorithms being k-means and hierarchical clustering, though other algorithms like Hidden Markov models, Self-Organizing Maps or Gaussian Mixture models are also often used. [94] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[111][112] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.  So long as we connect all these components together, we will have set up a reinforcement learning scenario to play the game Mario.\n",
       "Now that we’ve discussed the three different categories of machine learning, it’s important to note that a lot of times the lines between these types of learning blur. </p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(f\"<h3>{query}</h3>\\n<ul>{''.join([f'<li>{answer}</li>' for answer in answers])}</ul>\\n<p>{synth_text}</p>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
